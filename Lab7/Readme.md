## Cхема обучения агентов
• Агенты i = 1, ... , N
• Для агента окружающий мир – это остальные агенты и
содержащая агентов система.
• Последовательность периодов t = 1,2, ...
• У каждого агента есть модель окружающего мира с
параметрами θ_it.
События в течение одного периода t:
1. Каждый агент получает информацию об окружающем мире y_it
и корректирует параметры θ_it (обновляет модель).
2. Каждый агент решает на основе внутренней модели с
параметрами θ_it, какое действие x_it выбрать.
3. Каждый агент выполняет выбранные действия xit. Это
приводит к изменению состояний агентов и системы в целом.

## Онлайновое обучение одного агента

• У агента имеется метод прогнозирования (модель) для целевой
переменной y с параметрами θ: F(x, θ), где x – информация.
События для агента в течение одного периода t:
1. Узнает информацию xt и делает прогноз y^p_t целевой переменной y_t на основе информации x_t: y^p_t = F(x_t, θ_t).
2. Узнает фактическое значение y_t и получает потери l_t = L(y_t, y^p_t).
3. Корректирует параметры θ_t исходя из L(y_t, y^p_t).

## Простейшие алгоритмы обучения

В момент t знаем (y_1, ... , y_(t−1)). Делаем прогноз y^p_t для y_t.

• Простое накопленное среднее:
y^p_(t+1) =1/t * сумма по τ от 1 до t y_τ

• Среднее по M последним наблюдениям
y^p_(t+1) = 1/M * (сумма по τ от t−M+1 до t y_τ) = =1/M * (сумма по j от 1 до M y_(t−j+1))
.

• Взвешенное скользящее среднее (сумма весов: сумма по j от 1 до M w_j = 1):

y^p_(t+1) = сумма по j от 1 до M w_j * y_(t−j+1)

## Метод фиктивного разыгрывания в теории игр

Итеративный алгоритм поиска решения в матричных
антагонистических играх двух лиц. (Также называют методом Брауна–Робинсон)
Матричная антагонистическая игра двух лиц:
• У игрока i имеется m_i чистых стратегий.
• Выигрыши задаются платежной матрицей Π (m_1 × m_2).
• Игрок 1 выбирает строки матрицы (j_1), а игрок 2 – столбцы (j_2).
• При выборе j_1, j_2 выигрыш игрока 1 равен Π_(j_1 j_2), а выигрыш игрока 2 равен −Π_(j_1 j_2).

Смешанные стратегии: μ_1 = (μ_(1 1), ... , μ_(1 m_1)), μ_2 = (μ_21, ... , μ_(2 m_2)) – вероятности, с которыми игроки выбирают чистые стратегии.
Требуется найти равновесие (Нэша) в смешанных стратегиях.

## Стохастический градиентный спуск

Задача на минимум средних потерь
L̅(b) =1/t * сумма по t от 1 до T  L(y_t, x^T_t*b) → min(b).
Просто градиентный спуск: движемся в направлении, обратном градиенту ∇L̅(b) целевой функции L̅(b). Целевая функция в этом направлении уменьшается.
Вместо этого стохастический градиентный спуск:
b_t = bt−1 − ρ_t*∇L_t(b_(t−1)), где L_t(b) = L(y_t, x^T_t * b).
ρt > 0 – размер шага. Например, ρ_t ∝1/√t.
Вариант: b_t = b_(t−1) − ρ_t*Γ∇L_t(b_(t−1)), Γ – положит. определенная.
Если L(y_t, x^T_t * b) =1/2 *(y_t − x^T_t * b)^2, то ∇L_t(b) = −x^T_t(y_t − x^T_t * b). 
Тогда b_t = b_(t−1) + ρ_t * Γx^T_t(y_t − x*T_t * b_(t−1)) = bt−1 + ρ_t * Γx^T_t * e^P_t